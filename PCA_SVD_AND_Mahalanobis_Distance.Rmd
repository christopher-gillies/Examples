---
title: "PCA, SVD and Mahalanobis distance"
author: "Christopher Gillies"
date: "11/16/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(MASS)
require(ggplot2)
```

# Multivariate normal distribution

\begin{equation}
f(x)=\frac{1}{\sqrt{(2\pi)^n|\boldsymbol\Sigma|}}
\exp\left(-\frac{1}{2}({x}-{\mu})^T{\boldsymbol\Sigma}^{-1}({x}-{\mu})
\right)
\end{equation}

If we assume the data are zero-centered then:
\begin{equation}
f(x)=\frac{1}{\sqrt{(2\pi)^n|\boldsymbol\Sigma|}}
\exp\left(-\frac{1}{2}({x})^T{\boldsymbol\Sigma}^{-1}({x})
\right)
\end{equation}

#where $x$ is an $m$-dimensional vector.

## Generate a random sample

```{r}

S = matrix(c(1,0.4,0.4,2),ncol=2)
x = mvrnorm(n = 1000, mu=c(0,0), Sigma=S)

ggplot(data.frame()) + geom_point(aes(x=x[,1],y=x[,2])) + scale_x_continuous(limits=c(-5,5))  + scale_y_continuous(limits=c(-5,5)) +
  ggtitle("Random sample from a  bivariate normal distribution") + xlab("x1") + ylab("x2")
```

## Perform PCA
In principal component analysis, we compute the covariance of the features we are studying, and then compute the eigenvectors of this matrix.
Let a matrix $X \in \mathbb{R}^{n \times m}$ be a matrix, where we have $n$ observations for $m$ features.

\begin{equation}
X_{c} = X - {\mu_j}
\end{equation}

$X_c$ is the centered matrix of $X$, where we subtract the mean ($\mu_j$) of each column (feature) from the data and $j \in \{1,..m\}$.
 
```{r }
S.sample = cov(x)

e.decomp = eigen(S.sample)

e.decomp$vectors %*% diag(e.decomp$values) %*% t(e.decomp$vectors)

x.proj = x %*% e.decomp$vectors
ggplot(data.frame()) + geom_point(aes(x=x.proj[,1],y=x.proj[,2])) + scale_x_continuous(limits=c(-5,5))  + scale_y_continuous(limits=c(-5,5))

e.decomp$values
var(x.proj[,1])
var(x.proj[,2])
```

Notice that the variance of the projected data matches the eigenvalues of the covariance matrix of $X$. The projection of x onto its principal components simply rotates the data.

```{r }

svd.sample.cov = svd(S.sample)


x.center = scale(x,scale = FALSE)

svd.x = svd(x.center)


x.proj.2 = x.center %*% svd.x$v
x.proj.3 = x.center %*% svd.sample.cov$v

plot(x.proj.2[,1],x.proj.3[,1])
plot(x.proj.2[,2],x.proj.3[,2])

svd.x$d^2 / (1000 - 1)
svd.sample.cov$d
```

The same eigenvectors and eigenvalues are computed from the matrix x.center and the covariance matrix. The singlar values $\sigma_i$ of x and the eigenvalues $\lambda_i$ of COV$(X)$ are related as follows:

\begin{equation}
\lambda_i = \frac{\sigma_i^2}{n-1} 
\end{equation}

\begin{equation}
\text{COV} \left [ X \right ] = \frac{1}{n-1}X^TX 
\end{equation}

\begin{equation}
X = U \Sigma V^T 
\end{equation}

\begin{equation}
X^TX = (U \Sigma V^T)^T (U \Sigma V^T)
\end{equation}

\begin{equation}
X^TX = V \Sigma U^TU \Sigma V^T
\end{equation}

\begin{equation}
X^TX = V \Sigma \Sigma V^T = V \Sigma^2 V^T
\end{equation}

\begin{equation}
\frac{1}{n-1}X^TX = \frac{1}{n-1} V \Sigma^2 V^T \rightarrow \frac{1}{n-1} \Sigma^2 = \Lambda
\end{equation}

where $\Lambda$ is the diagonal matrix of eigenvalues of $\text{COV} \left [ X \right ]$. Also note that $V$ is a unitary matrix.

This is the same formula as above for the relationship between the singular values of X and the eigenvalues of its covariance matrix.

## What happens if we scale X before running PCA?

```{r }
x.scaled = scale(x)

cov.scaled.x = cov(x.scaled)
cov.scaled.x
S.sample

ggplot(data.frame()) + geom_point(aes(x=x[,1],y=x[,2])) + scale_x_continuous(limits=c(-5,5))  + scale_y_continuous(limits=c(-5,5)) + ggtitle("Before scaling")
ggplot(data.frame()) + geom_point(aes(x=x.scaled[,1],y=x.scaled[,2])) + scale_x_continuous(limits=c(-5,5))  + scale_y_continuous(limits=c(-5,5)) + ggtitle("After scaling")
```

```{r }
#this is equivalent to the correlation matrix
scaled.svd = svd(cov.scaled.x)

x.scaled.proj = x.scaled %*% scaled.svd$v 
ggplot(data.frame()) + geom_point(aes(x=x.scaled.proj[,1],y=x.scaled.proj[,2])) + scale_x_continuous(limits=c(-5,5))  + scale_y_continuous(limits=c(-5,5)) + ggtitle("After projection scaled x") 

svd.sample.cov$d
scaled.svd$d
```

It is interesting to note that the angle between the principal axes changes.

\begin{equation}
\cos(\theta) = \frac{v \cdot w}{\lVert v \rVert \lVert x \rVert}
\end{equation}

However, we know that the eigenvectors have already been normalized so we can rewrite the equation as:

\begin{equation}
\cos(\theta) = v \cdot w
\end{equation}

\begin{equation}
 \theta = \cos^{-1}(v \cdot w)
\end{equation}

```{r }
angle = function(v,w) {
  v.norm = v/sqrt(sum(v^2))
  w.norm = w / sqrt(sum(w^2))
  radians = acos(v.norm %*% w.norm)
  radians * 180 / pi
}
angle(c(1,0),-scaled.svd$v[,1])
angle(c(1,0),-scaled.svd$v[,2])
angle(c(1,0),svd.sample.cov$v[,1])
angle(c(1,0),-svd.sample.cov$v[,2])
```

So the angle between the principal axes changes significantly. In fact for a 2 by 2 matrix the eigenvectors of the correlation matrix are always the same.

## Data reconstruction

```{r }

x.projection = svd.sample.cov$v %*% t(x.center)
x.reconstruction = t(svd.sample.cov$v %*% x.projection)
x.reconstruction.partial = t(svd.sample.cov$v %*% rbind(x.projection[1,], 0))

cor(x.reconstruction[,1],x.center[,1])
cor(x.reconstruction[,2],x.center[,2])
cor(x.reconstruction[,1],x.reconstruction.partial[,1])
cor(x.reconstruction[,2],x.reconstruction.partial[,2])

```

Notince that the reconstruction works perfectly, and the partial reconstruction works fairly well.

## Whitening transform
```{r }
x.proj.white = x.center %*% svd.sample.cov$v %*% solve(diag(sqrt(svd.sample.cov$d)))
var(x.proj.white[,1])
var(x.proj.white[,2])
ggplot(data.frame()) + geom_point(aes(x=x.proj.white[,1],y=x.proj.white[,2])) + scale_x_continuous(limits=c(-4,4))  + scale_y_continuous(limits=c(-4,4)) + ggtitle("Whitening Transfrom")

w_sqrt = svd.sample.cov$v %*% diag(sqrt(svd.sample.cov$d)) %*% t(svd.sample.cov$v)
w_inv_sqrt = solve(w_sqrt)
w_inv_sqrt.2 = t(svd.sample.cov$v) %*% solve(diag(sqrt(svd.sample.cov$d))) %*% svd.sample.cov$v
x.proj.white.2= x.center %*% w_inv_sqrt.2
ggplot(data.frame()) + geom_point(aes(x=x.proj.white.2[,1],y=x.proj.white.2[,2])) + scale_x_continuous(limits=c(-4,4))  + scale_y_continuous(limits=c(-4,4)) + ggtitle("Whitening Transfrom 2")


cov(x.proj.white.2)
cov(x.proj.white)

```


The above are two different whitening transforms.
Please note that:
\begin{equation}
\text{COV}[X] = V \Lambda V^T
\end{equation}
In a diagonal matrix $\Lambda$ we can take the square roots by just taking the square root of the diagonal elements
\begin{equation}
\text{COV}[X] = (V \Lambda^{1/2} V^T)^2 = V \Lambda^{1/2} (V^T V) \Lambda^{1/2} V^T = V \Lambda V^T
\end{equation}
Since, $V^T V = I$. That is $V^T = V^{-1}$.
\begin{equation}
\text{COV}[X]^{1/2} = V \Lambda^{1/2} V^T 
\end{equation}


Now $(ABC)^-1 = C^{-1}B^{-1}A^{-1}$. Therefore
\begin{equation}
\text{COV}[X]^{-1/2} = (V \Lambda^{1/2} V^T )^{-1} = V^{-1} \Lambda^{-1/2} (V^{-1})^-1 = V^{T} \Lambda^{-1/2} V
\end{equation}
Since $V^T = V^{-1}$.

# Mahalanobis distance$^2$
The Mahalanobis of a centered vector from the origin is:
\begin{equation}
D_{M}(x)^2 = x^T S^{-1} x
\end{equation}
where $S$ is the covariance matrix of the vector $x$.

```{r }
t(x.center[1,]) %*% solve(S.sample) %*% x.center[1,]
sum(x.proj.white[1,]^2)
sum(x.proj.white.2[1,]^2)

x.1.proj.scaled = x.center[1,] %*% svd.sample.cov$v %*% solve(diag(sqrt(svd.sample.cov$d)))
sum(x.1.proj.scaled^2)
```

Mahalanobis distance$^2$ is the equal to the $\lVert x \rVert^2$ in the Whitened space.

The norm of a vector
\begin{equation}
\lVert x \rVert^2 = \sum{x_i^2} = x^Tx
\end{equation}
\begin{equation}
(x^T)^T = x
\end{equation}




\begin{equation}
D_{M}(x)^2 = x^T \text{COV}[X]^{-1} x
\end{equation}

\begin{equation}
\text{COV}[X]^{-1} = (V \Lambda V^T )^{-1} = V^T \Lambda^{-1} V
\end{equation}

The Mahalanobis distance$^2$ can be written as 
\begin{equation}
D_{M}(x)^2 = x^T V^T \Lambda^{-1} V x =  x^T V^T \Lambda^{-1/2}  \Lambda^{-1/2} V x = (x^T V^T \Lambda^{-1/2})  (\Lambda^{-1/2} V x) = (\Lambda^{-1/2} V x)^T (\Lambda^{-1/2} V x)
\end{equation}
\begin{equation}
= (\Lambda^{-1/2} V x)^T (\Lambda^{-1/2} V x) = \lVert \Lambda^{-1/2} V x \rVert^2 = \lVert x^T V^T \Lambda^{-1/2} \rVert^2 = D_{M}(x)^2 
\end{equation}
The final equality $\lVert x^T V^T \Lambda^{-1/2} \rVert^2$ shows that the $D_{M}(x)^2$ is the same as projecting the $x$ onto its principal components, then scaling each axis by the square root of its eigenvalue (if the eigenvalue is the variance then the sqrt(eigenvalue) is like the standard deviation) and finally taking the norm of the scaled projected x. A eigenvalue is the variance of the data projected onto its corresponding eigenvector, so to scale it you divide by the standard deviation.

# What about PCA and SVD in the case where there are more variables than observations?

Generate random samples. Assume we have $n=100$ subjects and $m=200$ snps.
```{r }
n = 100
m = 1000
z = rbinom(n,size = 2, prob = 0.5)
X = matrix(rep(0,n * m),ncol=n)
snp_afs_0 = runif(m,0.01,0.5)
snp_afs_1 = runif(m,0.25,0.75)
snp_afs_2 = c(snp_afs_0[1:(m/2)],snp_afs_1[(m/2 + 1):m])
for(i in 1:n) {
  x_i = NULL
  if(z[i] == 0) {
    x_i = rbinom(m,size=2,prob=snp_afs_0)
  } else if(z[i] == 1) {
    x_i = rbinom(m,size=2,prob=snp_afs_1)
  } else {
    x_i = rbinom(m,size=2,prob=snp_afs_2)
  }
  X[,i] = x_i
}
```

Standardize by subtracting off the row means and dividing by the standard deviation. Also compute covariance matrix.
```{r }
X_std = t(scale(t(X)))
ind_cov = cov(X_std)
ind_cor_x = cor(X)

#approx covariance matrix
Xt_X = t(X_std) %*% X_std * 1/(n-1)

dim(ind_cov)
dim(Xt_X)

sign(Xt_X[1:10,1:10]) == sign(ind_cov[1:10,1:10])
Xt_X[1:10,1:10] / ind_cov[1:10,1:10]
```

Now let us compare the eigenvalues of each
```{r }
eigen.XT_X = eigen(Xt_X)
eigen.ind_cov = eigen(ind_cov)
eigen.ind_cor = eigen(cor(X_std))
eigen.ind_cor_x = eigen(ind_cor_x)
svd.X = svd(X_std)

color = function(x) {
  a_func = function(a) {
    if(a == 0) {
      return("black")
    } else if(a == 1) {
      return("red")
    } else {
      return("blue")
    }
  }
  sapply(x,FUN=a_func)
}

(svd.X$d^2 / (n - 1))[1:10]
eigen.XT_X$values[1:10]
plot(svd.X$d^2 / (n - 1),eigen.XT_X$values,main="singluar values^2 / (n-1) = eigenvalues")
plot(svd.X$v[,1],svd.X$v[,2],col=color(z))
plot(eigen.XT_X$vectors[,1],eigen.XT_X$vectors[,2],col=color(z))
plot(eigen.ind_cov$vectors[,1],eigen.ind_cov$vectors[,2],col=color(z))
plot(eigen.ind_cor$vectors[,1],eigen.ind_cor$vectors[,2],col=color(z))
plot(eigen.ind_cor_x$vectors[,1],eigen.ind_cor_x$vectors[,2],col=color(z))

plot(eigen.ind_cov$vectors[,1],eigen.XT_X$vectors[,1])
plot(eigen.ind_cov$vectors[,2],eigen.XT_X$vectors[,2])
plot(eigen.ind_cov$vectors[,3],eigen.XT_X$vectors[,3])


plot(eigen.ind_cor$vectors[,1],eigen.XT_X$vectors[,1])
plot(eigen.ind_cor$vectors[,2],eigen.XT_X$vectors[,2])
plot(eigen.ind_cor$vectors[,3],eigen.XT_X$vectors[,3])

plot(eigen.ind_cov$values,eigen.XT_X$values)
plot(eigen.ind_cov$values,eigen.ind_cor$values)
plot(eigen.ind_cor$values,eigen.ind_cor_x$values)

plot(eigen.ind_cor_x$vectors[,1],eigen.XT_X$vectors[,1])
plot(eigen.ind_cor_x$vectors[,2],eigen.XT_X$vectors[,2])


```
The eigenvalues from svd match those from the eigenvalues from the matrix Xt_X

Now let us see how 



Higher varying genes have higher weights assigned to them
```{r }
plot((svd.X$u[,1])^2,apply(X,MARGIN=1,FUN=var))
cor((svd.X$u[,1])^2,apply(X,MARGIN=1,FUN=var))

best_snps_for_comp_1 = sort(svd.X$u[,1],decreasing=T,index.return=T)$ix
best_snps_for_comp_2 = sort(svd.X$u[,2],decreasing=T,index.return=T)$ix

genes_ranked_by_var = sort(apply(X,MARGIN=1,FUN=var),index.return=T,decreasing=T)
ggplot(data.frame()) + geom_jitter(aes(x=X[best_snps_for_comp_1[1],],y=X[best_snps_for_comp_2[1],],color=color(z)),width = 0.25,height=0.25)
ggplot(data.frame()) + geom_jitter(aes(x=X[best_snps_for_comp_2[1],],y=X[best_snps_for_comp_2[2],],color=color(z)),width = 0.25,height=0.25)

ggplot(data.frame()) + geom_density(aes(x=X[best_snps_for_comp_1[1],],color=color(z)))
ggplot(data.frame()) + geom_density(aes(x=X[best_snps_for_comp_1[2],],color=color(z)))
ggplot(data.frame()) + geom_density(aes(x=X[best_snps_for_comp_2[1],],color=color(z)))
ggplot(data.frame()) + geom_density(aes(x=X[best_snps_for_comp_2[2],],color=color(z)))
```

# How does centering affect the calculation of eigenvectors?
```{r }
X_row_center = X - apply(X,MARGIN=1,FUN=mean)
X_col_center = X - apply(X,MARGIN=2,FUN=mean)

X_row_center_svd = svd(X_row_center)
X_col_center_svd = svd(X_col_center)
```

\begin{equation}
X = U \Sigma V^{T}
\end{equation}

\begin{equation}
U^{T} X = \Sigma V^{T} \rightarrow X^{T}U = V \Sigma^{T} \rightarrow X^{T}U = V \Sigma
\end{equation}

In this case, the columns of $U$ are the eigenvectors of $XX^T$. $XX^T = U\Sigma^2U^T$ which is proportional to the covariance matrix between SNPs. In this case, $V \Sigma$ contains the principal component scores of, which are the subject's projected into the space of $U$.
```{r }
X_XT_row_center = X_row_center %*% t(X_row_center)
X_XT_row_center_svd = svd(X_XT_row_center)

X_XT_row_center_svd$v[1:10,1:10]
X_XT_row_center_svd$u[1:10,1:10]
```


```{r }
X_XT_row_center_svd$u[1:10,1:10]
X_row_center_svd$u[1:10,1:10]
```
From the above, we can see that the eigenvectors match.

Now we can see that the projection and the PC scores match.
```{r }
X_row_center_proj = t(X_row_center) %*% X_row_center_svd$u
X_row_center_proj_2 = X_row_center_svd$v %*% diag(X_row_center_svd$d)

X_row_center_proj[1:10,1:10]
X_row_center_proj_2[1:10,1:10]

ggplot(data.frame()) + geom_point(aes(x=X_row_center_proj[,1],X_row_center_proj[,2],color=factor(z))) + ggtitle("Projection of X onto the eigenvectors of the covariance matrix of the snps")
```

From this we can see that the columns of $U$ specify the weights of each SNP for each eigenvector.

\begin{equation}
X = U \Sigma V^{T}
\end{equation}

\begin{equation}
X V = U \Sigma
\end{equation}

In this case, the columns of $V$ are the eigenvectors of $X^TX$. In this case, we are projecting the SNPs onto the eigenvectors of the covariance matrix of the individuals. So $U \Sigma$ contain the coordinates of each gene in the space of the principal components of $V$.

```{r }
X_col_center_proj = X_col_center %*% X_col_center_svd$v
X_col_center_proj_2 = X_col_center_svd$u %*% diag(X_col_center_svd$d)

X_col_center_proj[1:10,1:10]
X_col_center_proj_2[1:10,1:10]

plot(X_col_center_proj[,1],X_col_center_proj[,2])
```

# Conclusion on how to do PCA when number of samples are less than the number of features.
Let $X$ be a matrix of $m$ rows of features and $n$ columns of samples. Center and/or standardize each colum, then compute the SVD of $X = U \Sigma V^T$. $U$ are the eigenvectors of $XX^T$ (proportional to the m by m covariance matrix of features). Each column of $U$ is an eigenvector of the covariance matrix. $U^T X =  \Sigma V^T \rightarrow X^TU = V \Sigma$. $U_{i,j}$ is the weight of feature $i$ in eigenvector $j$. Column $j$ of $X^T U$ or $V \Sigma$ are the scores of all individuals along the $j$ principal component.

So ultimately it comes down to are we interested in seeing the relationship between features or between individuals.

# How do different covariance matrices affect the calculation of different Mahalanobis distances?


```{r }
cov.mat = matrix(
  c(1,   -0.5,
    -0.5, 1),ncol=2)

eigen.decomp = eigen(cov.mat)

vals = c(-2,1)
sqrt(vals %*% solve(cov.mat) %*% vals)

#distance from origin in white transformed space
vals.proj.scaled = vals %*% eigen.decomp$vectors / sqrt(eigen.decomp$values)
sqrt(sum(vals.proj.scaled^2))
angle(c(1,0),t(vals.proj.scaled))
sum(vals.proj.scaled)

vals = c(2,2)
sqrt(vals %*% solve(cov.mat) %*% vals)


vals = c(-1,3)
sqrt(vals %*% solve(cov.mat) %*% vals)

#distance from origin in white transformed space
vals.proj.scaled = vals %*% eigen.decomp$vectors / sqrt(eigen.decomp$values)
sqrt(sum(vals.proj.scaled^2))
```

```{r }
cov.mat = matrix(
  c(4,   4*0.25,
    4*0.25, 4),ncol=2)

vals = c(4,-4)
sqrt(vals %*% solve(cov.mat) %*% vals)

cov.mat = matrix(
  c(1,   0.25,
    0.25, 1),ncol=2)

vals = c(2,-2)
sqrt(vals %*% solve(cov.mat) %*% vals)

cov.mat = matrix(
  c(1,   -0.25,
    -0.25, 1),ncol=2)

vals = c(2,-2)
sqrt(vals %*% solve(cov.mat) %*% vals)

cov.mat = matrix(
  c(1,   0,
    0, 1),ncol=2)

vals = c(3,3)
sqrt(vals %*% solve(cov.mat) %*% vals)

cov.mat = matrix(
  c(1,   0.9,
    0.9, 1),ncol=2)

vals = c(3,3)
sqrt(vals %*% solve(cov.mat) %*% vals)
```

```{r }
cov.mat = matrix(
  c(1,   0.5, -0.75,
    0.5, 1,    0.5,
   -0.75, 0.5,  1),ncol=3)

vals = c(-3,0,3)
sqrt(vals %*% solve(cov.mat) %*% vals)
```
